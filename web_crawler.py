import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def gather_urls(starting_url, max_depth=2):
    visited_urls = set()
    urls_to_visit = [(starting_url, 0)]  # (url, depth)

    gathered_urls = []

    while urls_to_visit:
        current_url, current_depth = urls_to_visit.pop(0)
        if current_depth > max_depth:
            continue

        try:
            response = requests.get(current_url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            for link in soup.find_all('a', href=True):
                full_url = urljoin(current_url, link['href'])
                parsed_url = urlparse(full_url)

                # Normalize the URL to avoid duplicate URLs
                normalized_url = parsed_url.scheme + "://" + parsed_url.netloc + parsed_url.path

                if normalized_url not in visited_urls:
                    visited_urls.add(normalized_url)
                    gathered_urls.append(normalized_url)

                    # Add to queue if within the same domain and depth allows
                    if parsed_url.netloc == urlparse(starting_url).netloc:
                        urls_to_visit.append((normalized_url, current_depth + 1))

        except requests.RequestException as e:
            print(f"Failed to access {current_url}: {e}")

    return gathered_urls
